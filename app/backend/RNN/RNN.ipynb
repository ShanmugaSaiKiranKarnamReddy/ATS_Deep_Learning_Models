{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "Verbose = False\n",
    "if Verbose:\n",
    "    def vprint(*args, **kwargs): print(*args, **kwargs, flush=True)\n",
    "else: # do-nothing function\n",
    "    def vprint(*args, **kwargs): None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.width\", 380)\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pair_ID                                                                                               resume                                                                                                   jd score\n",
      "0           1                              Bachelor of ScienceComputer Science & Mathematics North Central College    B.S. in Computer Science or equivalent industrial experience required; Masters Degree preferred.  four\n",
      "1           2                                                                 onetwo years in Java/Web development                          one0+ years of overall software development experience, five+ years in Java  five\n",
      "2           3                                                                        Algorithm and Data Structures  Proficiency in coding, data structures, algorithms, and designing for performance, scalability, ...  four\n",
      "3           4                    Students evaluations of my performance noted my patience and clear communication                          Strong analytical skills, excellent communication and interpersonal skills.  five\n",
      "4           5                          Replaced company website with a more trustworthy and better performing site  Experience in building security features for large-scale systems, specifically around data encry...   two\n",
      "...       ...                                                                                                  ...                                                                                                  ...   ...\n",
      "1137     1138  Developed a web app for show casing how this two-way audio works using HTML, CSS and React JS. \\...     Develop a flexible and well-structured front-end architecture, along with the APIs to support it  five\n",
      "1138     1139                                                                 Tested REST endpoints using POSTMAN.                                                                           Experience with REST API's  five\n",
      "1139     1140                                                       GitHub and Git Lab as version control systems.                                                                              Git knowledge is a plus  five\n",
      "1140     1141  University of Illinois At Springfield - Springfield, IL0five/two0one7 \\nMaster of Science: Compu...                                     Bachelors degree in any STEM program (or equivalent experience)  five\n",
      "1141     1142  Enforced Agile and Scrum development methodologies on interface projects to shorten development ...                                                                     Participate in agile development  five\n",
      "\n",
      "[1142 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dev = pd.read_csv(\"C:/Users/Rajath/Downloads/sentence-entailment-master/sentence-entailment-master/test2.txt\", sep = '\\t', encoding = \"ISO-8859-1\")\n",
    "df_train = pd.read_csv(\"C:/Users/Rajath/Downloads/sentence-entailment-master/sentence-entailment-master/train1.txt\",sep ='\\t', encoding = \"ISO-8859-1\")\n",
    "#df_dev = pd.read_csv(\"C:/Users/Rajath/Downloads/sentence-entailment-master/sentence-entailment-master/test4.csv\",sep ='$', encoding = \"ISO-8859-1\")\n",
    "\n",
    "print(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pair_ID                                                                                               resume                                                                                                jd  score\n",
      "0          1                 Knowledge of analyzing, troubleshooting, and providing solutions to technical issues                Experience analyzing, troubleshooting, and providing solutions to technical issues   four\n",
      "1          2                                                             three+ years of experience in IT support                            two+ years hands-on IT Support experience in a production environment   four\n",
      "2          3                                                 three+ years of experience on windows administration                             two+ years' experience supporting Microsoft Windows 7 or Windows one0  three\n",
      "3          4                                                   Previous experience on PC repair, troubleshooting,                          Proven skills in PC repair, troubleshooting, deployment, and liquidation  three\n",
      "4          5                                                              application support for windows systems                                           Experience supporting one or more of: Mac OS X or Linux  three\n",
      "..       ...                                                                                                  ...                                                                                               ...    ...\n",
      "137      138  Developed a web app for show casing how this two-way audio works using HTML, CSS and React JS. \\...  Develop a flexible and well-structured front-end architecture, along with the APIs to support it   five\n",
      "138      139                                                                 Tested REST endpoints using POSTMAN.                                                                        Experience with REST API's   five\n",
      "139      140                                                       GitHub and Git Lab as version control systems.                                                                           Git knowledge is a plus   five\n",
      "140      141  University of Illinois At Springfield - Springfield, IL0five/two0one7 \\nMaster of Science: Compu...                                  Bachelors degree in any STEM program (or equivalent experience)   five\n",
      "141      142  Enforced Agile and Scrum development methodologies on interface projects to shorten development ...                                                                  Participate in agile development   five\n",
      "\n",
      "[142 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "print(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ats(Dataset):\n",
    "    endOfSentence   = '</s>'\n",
    "    startOfSentence = '<s>'\n",
    "    separator2Sentences = '<sep>'\n",
    "    \n",
    "    text_label = [\"one\", \"two\", \"three\",\"four\",\"five\"]\n",
    "    \n",
    "    tokens = [startOfSentence, separator2Sentences, endOfSentence]\n",
    "    \n",
    "    def join_sentence(self, row):\n",
    "        \"\"\"\n",
    "        Create a new sentence (<s> + s_A + <sep> + s_B + </s>)\n",
    "        \"\"\"\n",
    "        resume = str(row['resume']).split(\" \")\n",
    "        jd = str(row['jd']).split(\" \")\n",
    "        return np.concatenate((\n",
    "            [self.startOfSentence],\n",
    "            resume,\n",
    "            [self.separator2Sentences],\n",
    "            jd,\n",
    "            [self.endOfSentence]\n",
    "        ))\n",
    "    \n",
    "    def series_text_2_labelID(self, series, keep_n=1000):\n",
    "        \"\"\"\n",
    "        Convert text Label into label id\n",
    "        \"\"\"\n",
    "        reverse_dict = {v: k for k, v in  dict(enumerate(self.text_label)).items()}\n",
    "        return series.map(reverse_dict)\n",
    "    \n",
    "    def series_2_dict(self, series, keep_n):\n",
    "        \"\"\"\n",
    "        Convert document (a list of words) into a list of indexes\n",
    "        AND apply some filter on the documents\n",
    "        \"\"\"\n",
    "        dictionary = corpora.Dictionary(series)\n",
    "        dictionary.filter_extremes(\n",
    "            no_below=1,\n",
    "            no_above=1,\n",
    "            keep_n=keep_n,\n",
    "            keep_tokens=self.tokens)\n",
    "        return dictionary\n",
    "    \n",
    "    \n",
    "    def __init__(self, df, vocabulary_size, dic=None):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "        # Label text as ids\n",
    "        df[\"score_id\"] = self.series_text_2_labelID(df['score'])\n",
    "        \n",
    "        # Add <s>,</s>,<sep> tokens to the vocabulary\n",
    "        df['sentence_AB'] = df.apply(self.join_sentence, axis=1)\n",
    "        \n",
    "        # check if the dictionary is given\n",
    "        if dic is None:\n",
    "            # Create the Dictionary\n",
    "            self.dictionary = self.series_2_dict(df['sentence_AB'], vocabulary_size)\n",
    "        else:\n",
    "            self.dictionary = dic\n",
    "        \n",
    "        # sentence of words -> array of idx\n",
    "        # Adds unknown to the voc (idx = len(dictionary)), len(dictionary) = vocabulary_size\n",
    "        # Adds one to each (no tokens at 0, even <unk>)\n",
    "        # 0 is for the padding when using mini-batch\n",
    "        df[\"word_idx\"] = df[\"sentence_AB\"].apply(\n",
    "            lambda word: np.array(self.dictionary.doc2idx(word, unknown_word_index=vocabulary_size)) + 1\n",
    "        )\n",
    "        \n",
    "        self.df = df\n",
    "\n",
    "        # compute a sorted occurence dictionary on the whole corpus\n",
    "        occ_dict = {}\n",
    "        for serie in df['sentence_AB']:\n",
    "            unique, counts = np.unique(serie, return_counts=True)\n",
    "            tmp_dict = dict(zip(unique, counts))\n",
    "            \n",
    "            for key, value in tmp_dict.items():\n",
    "                if key in occ_dict:\n",
    "                    occ_dict[key] = occ_dict[key] + tmp_dict[key]\n",
    "                else:\n",
    "                    occ_dict[key] = value\n",
    "        \n",
    "        self.occ_dict_list = [[key, value] for key, value in occ_dict.items()]\n",
    "        self.occ_dict_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    def getSortedOccDictList(self):\n",
    "        return self.occ_dict_list\n",
    "    \n",
    "    def plotVocabularyCoverage(self):\n",
    "        occdict_list = self.occ_dict_list\n",
    "        \n",
    "        total = 0\n",
    "        y = []\n",
    "        for i, value in enumerate(occdict_list):\n",
    "            total += value[1]\n",
    "            y.append(total)\n",
    "            if (i == self.vocabulary_size):\n",
    "                current_voc_cov = total\n",
    "\n",
    "        current_voc_cov = current_voc_cov*100.0/total\n",
    "        \n",
    "        y = [tmp*100.0/total for tmp in y]\n",
    "\n",
    "        x = np.linspace(0, len(occdict_list), len(occdict_list))\n",
    "\n",
    "        # Show graph\n",
    "        fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "        fig_size[0] = 20\n",
    "        fig_size[1] = 9\n",
    "        plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "        legend, = plt.plot(x, y, label='Vocabulary size ')\n",
    "\n",
    "        plt.title(('Current vocabulary size n=' + str(self.vocabulary_size) + ' coverage = ' +\"{:.4}\".format(current_voc_cov) + '%'),\n",
    "                     fontsize=14, fontweight='bold', color='gray')\n",
    "        plt.suptitle(('Vocabulary coverage'),\n",
    "                     fontsize=24, fontweight='bold', color='gray')\n",
    "        plt.xlabel(\"Size of unique vocabulary\", color='gray', fontsize=14)\n",
    "        plt.ylabel(\"Vocabulary coverage %\", color='gray', fontsize=14)\n",
    "\n",
    "        ## Plot Swagg ##\n",
    "        plt.yticks(fontsize=14, rotation=0, color='gray')\n",
    "        plt.xticks(fontsize=14, rotation=0, color='gray')\n",
    "\n",
    "        # Less border\n",
    "        plt.gca().xaxis.grid(True)\n",
    "        plt.gca().yaxis.grid(True)\n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n",
    "        plt.gca().spines['left'].set_visible(False)\n",
    "        plt.gca().spines['bottom'].set_visible(False)\n",
    "        plt.show()\n",
    "        \n",
    "    def getRef(self, index):\n",
    "        return self.df['sentence_AB'][index]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.df['word_idx'][index],\n",
    "            self.df['score_id'][index])\n",
    "    \n",
    "    def getDictionary(self):\n",
    "        return self.dictionary\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "vocabulary_size = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>resume</th>\n",
       "      <th>jd</th>\n",
       "      <th>score</th>\n",
       "      <th>score_id</th>\n",
       "      <th>sentence_AB</th>\n",
       "      <th>word_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bachelor of ScienceComputer Science &amp; Mathematics North Central College</td>\n",
       "      <td>B.S. in Computer Science or equivalent industrial experience required; Masters Degree preferred.</td>\n",
       "      <td>four</td>\n",
       "      <td>3</td>\n",
       "      <td>[&lt;s&gt;, Bachelor, of, ScienceComputer, Science, &amp;, Mathematics, North, Central, College, &lt;sep&gt;, B....</td>\n",
       "      <td>[3, 6, 20, 15, 14, 1, 12, 13, 7, 8, 4, 5, 18, 9, 14, 21, 16, 19, 17, 23, 11, 10, 22, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>onetwo years in Java/Web development</td>\n",
       "      <td>one0+ years of overall software development experience, five+ years in Java</td>\n",
       "      <td>five</td>\n",
       "      <td>4</td>\n",
       "      <td>[&lt;s&gt;, onetwo, years, in, Java/Web, development, &lt;sep&gt;, one0+, years, of, overall, software, deve...</td>\n",
       "      <td>[3, 30, 33, 18, 25, 26, 4, 29, 33, 20, 31, 32, 26, 27, 28, 33, 18, 24, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Algorithm and Data Structures</td>\n",
       "      <td>Proficiency in coding, data structures, algorithms, and designing for performance, scalability, ...</td>\n",
       "      <td>four</td>\n",
       "      <td>3</td>\n",
       "      <td>[&lt;s&gt;, Algorithm, and, Data, Structures, &lt;sep&gt;, Proficiency, in, coding,, data, structures,, algo...</td>\n",
       "      <td>[3, 1501, 38, 34, 36, 4, 35, 18, 39, 40, 45, 37, 38, 41, 42, 43, 44, 38, 1501, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Students evaluations of my performance noted my patience and clear communication</td>\n",
       "      <td>Strong analytical skills, excellent communication and interpersonal skills.</td>\n",
       "      <td>five</td>\n",
       "      <td>4</td>\n",
       "      <td>[&lt;s&gt;, Students, evaluations, of, my, performance, noted, my, patience, and, clear, communicatio...</td>\n",
       "      <td>[3, 1501, 1501, 20, 52, 53, 1501, 52, 1501, 38, 48, 49, 4, 46, 47, 54, 50, 49, 38, 51, 55, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Replaced company website with a more trustworthy and better performing site</td>\n",
       "      <td>Experience in building security features for large-scale systems, specifically around data encry...</td>\n",
       "      <td>two</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;s&gt;, Replaced, company, website, with, a, more, trustworthy, and, better, performing, site, &lt;se...</td>\n",
       "      <td>[3, 1501, 61, 72, 73, 57, 66, 1501, 38, 59, 67, 69, 4, 56, 18, 60, 68, 62, 42, 64, 71, 70, 58, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID                                                                             resume                                                                                                   jd score  score_id                                                                                          sentence_AB  \\\n",
       "0        1            Bachelor of ScienceComputer Science & Mathematics North Central College    B.S. in Computer Science or equivalent industrial experience required; Masters Degree preferred.  four         3  [<s>, Bachelor, of, ScienceComputer, Science, &, Mathematics, North, Central, College, <sep>, B....   \n",
       "1        2                                               onetwo years in Java/Web development                          one0+ years of overall software development experience, five+ years in Java  five         4  [<s>, onetwo, years, in, Java/Web, development, <sep>, one0+, years, of, overall, software, deve...   \n",
       "2        3                                                      Algorithm and Data Structures  Proficiency in coding, data structures, algorithms, and designing for performance, scalability, ...  four         3  [<s>, Algorithm, and, Data, Structures, <sep>, Proficiency, in, coding,, data, structures,, algo...   \n",
       "3        4  Students evaluations of my performance noted my patience and clear communication                          Strong analytical skills, excellent communication and interpersonal skills.  five         4  [<s>, Students, evaluations, of, my, performance, noted, my, patience, and, clear, communicatio...   \n",
       "4        5        Replaced company website with a more trustworthy and better performing site  Experience in building security features for large-scale systems, specifically around data encry...   two         1  [<s>, Replaced, company, website, with, a, more, trustworthy, and, better, performing, site, <se...   \n",
       "\n",
       "                                                                                              word_idx  \n",
       "0              [3, 6, 20, 15, 14, 1, 12, 13, 7, 8, 4, 5, 18, 9, 14, 21, 16, 19, 17, 23, 11, 10, 22, 2]  \n",
       "1                            [3, 30, 33, 18, 25, 26, 4, 29, 33, 20, 31, 32, 26, 27, 28, 33, 18, 24, 2]  \n",
       "2                    [3, 1501, 38, 34, 36, 4, 35, 18, 39, 40, 45, 37, 38, 41, 42, 43, 44, 38, 1501, 2]  \n",
       "3        [3, 1501, 1501, 20, 52, 53, 1501, 52, 1501, 38, 48, 49, 4, 46, 47, 54, 50, 49, 38, 51, 55, 2]  \n",
       "4  [3, 1501, 61, 72, 73, 57, 66, 1501, 38, 59, 67, 69, 4, 56, 18, 60, 68, 62, 42, 64, 71, 70, 58, 4...  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the train dataset\n",
    "dataset_train = ats(df_train, vocabulary_size)\n",
    "dataset_train.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dev dataset\n",
    "dictionary_train = dataset_train.getDictionary()\n",
    "\n",
    "dataset_dev = ats(df_dev, vocabulary_size, dictionary_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset\n",
    "\n",
    "#dataset_test = ats(df_test, vocabulary_size, dictionary_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support</td>\n",
       "      <td>&lt;sep&gt;</td>\n",
       "      <td>Lead</td>\n",
       "      <td>and</td>\n",
       "      <td>influence</td>\n",
       "      <td>technical</td>\n",
       "      <td>direction</td>\n",
       "      <td>and</td>\n",
       "      <td>roadmap</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1501</td>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1     2    3          4          5          6    7        8     9\n",
       "0  Support  <sep>  Lead  and  influence  technical  direction  and  roadmap  </s>\n",
       "1        3   1501    38   34         36          4         35   18       39    40"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(dataset_train.getRef(6)[-10:], dataset_train[2][0]))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained embeddings\n",
    "https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754 746\n",
      "Downloaded: Pretained Embedding matrix: torch.Size([400000, 50])\n",
      "Adapted:    Pretained Embedding matrix: torch.Size([1502, 50])\n"
     ]
    }
   ],
   "source": [
    "from torchtext import vocab\n",
    "#vocab is shared across all the text fields\n",
    "#CAUTION: GloVe will download all embeddings locally (862 MB).\n",
    "pretrained_emb = vocab.GloVe(name='6B', dim=EMBEDDINGS_SIZE)\n",
    "\n",
    "# 0 is for the padding when using mini-batch (start at one, shift by one)\n",
    "weights_matrix = np.zeros((vocabulary_size + 2, EMBEDDINGS_SIZE)) # do not forget the unk\n",
    "\n",
    "found = 0\n",
    "no_found = 0\n",
    "# build a matrix of weights that will be loaded into the PyTorch embedding layer\n",
    "for word_id in dataset_train.dictionary:\n",
    "    word = dataset_train.dictionary[word_id]\n",
    "    if word in pretrained_emb.stoi:\n",
    "        pretrained_emb_ID = pretrained_emb.stoi[word]\n",
    "        \n",
    "        weights_matrix[word_id+1] = pretrained_emb.vectors[pretrained_emb_ID]\n",
    "        found += 1\n",
    "    else:\n",
    "        weights_matrix[word_id+1] = np.random.normal(scale=0.6, size=(EMBEDDINGS_SIZE, ))\n",
    "        no_found += 1\n",
    "        \n",
    "# UNK\n",
    "weights_matrix[vocabulary_size+1] = np.random.normal(scale=0.6, size=(EMBEDDINGS_SIZE, ))\n",
    "\n",
    "print(found, no_found)\n",
    "        \n",
    "pretrained_emb_vec = torch.tensor(weights_matrix, dtype=torch.float32)\n",
    "print(\"Downloaded: Pretained Embedding matrix: \" +  str(pretrained_emb.vectors.size()))\n",
    "print(\"Adapted:    Pretained Embedding matrix: \" +  str(pretrained_emb_vec.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87, 153]\n",
      "in\n",
      "tensor([ 0.3304,  0.2500, -0.6087,  0.1092,  0.0364,  0.1510, -0.5508, -0.0742,\n",
      "        -0.0923, -0.3282,  0.0960, -0.8227, -0.3672, -0.6701,  0.4291,  0.0165,\n",
      "        -0.2357,  0.1286, -1.0953,  0.4333,  0.5707, -0.1036,  0.2042,  0.0783,\n",
      "        -0.4279, -1.7984, -0.2786,  0.1195, -0.1269,  0.0317,  3.8631, -0.1779,\n",
      "        -0.0824, -0.6270,  0.2650, -0.0572, -0.0735,  0.4610,  0.3086,  0.1250,\n",
      "        -0.4861, -0.0080,  0.0312, -0.3658, -0.4270,  0.4216, -0.1167, -0.5070,\n",
      "        -0.0273, -0.5329])\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.dictionary.doc2idx([\"the\", \"The\"]))\n",
    "print(dataset_train.dictionary[17])\n",
    "print(pretrained_emb_vec[17+1])\n",
    "# Glove dim=50 word=the vector[:4] = 0.418 0.24968 -0.41242 0.1217"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    seqs_labels = np.array(batch)[:,1]\n",
    "    \n",
    "    vectorized_seqs = np.array(batch)[:,0]\n",
    "    seq_lengths = torch.LongTensor([len(x) for x in vectorized_seqs])\n",
    "    \n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "              \n",
    "    vectorized_seqs = np.array(seq_tensor)\n",
    "    \n",
    "    return torch.tensor(vectorized_seqs), torch.LongTensor([ x for x in seqs_labels])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset_train,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "dev_loader = DataLoader(dataset=dataset_dev,\n",
    "                         batch_size=1, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "#test_loader = DataLoader(dataset=dataset_test,\n",
    " #                         batch_size=1, shuffle=False)\n",
    "\n",
    "# Debug the padding\n",
    "# display([ x for x in enumerate(train_loader)][0]) # has padding (sample of same size padded with 0)\n",
    "# display([ x for x in enumerate(dev_loader)][0]) # no batch == no padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import itertools\n",
    "import io\n",
    "from tensorboardX.utils import figure_to_image\n",
    "import matplotlib\n",
    "\n",
    "def confusion_scores(total_labels, total_pred, writer=None):\n",
    "    #fig = plt.figure(figsize=(10,10))\n",
    "    classes = ats.text_label\n",
    "    a = 1\n",
    "    for i in total_pred:\n",
    "        print(\"Resume\",  a ,\" got a score of Score of:\", int(i+1))\n",
    "        a = a + 1\n",
    "    title='Confusion matrix'\n",
    "    \n",
    "    cm = confusion_matrix(total_labels, total_pred, labels=[0, 1, 2,3,4])\n",
    "    \n",
    "   # plt.rcParams[\"figure.figsize\"] = [10, 10]\n",
    "   # plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "   # plt.title(title, color='gray', fontsize=24)\n",
    "   # plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "   # plt.xticks(tick_marks, [c.lower() for c in classes], rotation=45 , style='italic', color='gray', fontsize=17)\n",
    "   # plt.yticks(tick_marks, [c.lower() for c in classes], color='gray', style='italic', fontsize=17)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    #for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    #    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "    #             horizontalalignment=\"center\",\n",
    "    #             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.ylabel('True label', color='gray', fontsize=19)\n",
    "    #plt.xlabel('Predicted label', color='gray', fontsize=19)\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    #if writer != None:\n",
    "     #   writer.add_figure('plt/confusion_matrix', fig, 0)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, whileTraining=True, criterion=None, writer=None):\n",
    "    \"\"\"\n",
    "    Displays the confusion_matrix the precision recall fscore\n",
    "    If in whileTrainnig Mode only return the accuracy and loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_labels = torch.LongTensor([])\n",
    "        total_pred = torch.LongTensor([])\n",
    "        train_loss_batches = 0\n",
    "        train_loss_batches_count = 0\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                output = rnn(data)\n",
    "\n",
    "                \n",
    "                if whileTraining and criterion != None:\n",
    "                    loss = criterion(output, target)\n",
    "                    train_loss_batches +=loss.cpu().detach().numpy()\n",
    "                    train_loss_batches_count += 1\n",
    "\n",
    "                # Get the Accuracy\n",
    "                _, predicted = torch.max(output.data, dim=1)\n",
    "                correct = (predicted == target).sum().item()\n",
    "                \n",
    "                total_labels = torch.cat((total_labels, target.cpu()))\n",
    "                total_pred = torch.cat((total_pred, predicted.cpu()))\n",
    "                \n",
    "                \n",
    "        model.train()\n",
    "        if whileTraining and criterion!=None:\n",
    "            return ((accuracy_score(total_labels.flatten().numpy(), total_pred.flatten().numpy()) * 100), train_loss_batches / train_loss_batches_count)\n",
    "\n",
    "                \n",
    "        confusion_scores(total_labels, total_pred, writer=writer)\n",
    "        \n",
    "        #print(\"Accuracy:  {:.4f}\".format(accuracy_score(total_labels, total_pred)))\n",
    "        \n",
    "        # compute per-label precisions, recalls, F1-scores, and supports instead of averaging \n",
    "        metrics = precision_recall_fscore_support(\n",
    "                                        total_labels, total_pred,\n",
    "                                        average=None, labels=[0, 1, 2, 3, 4])\n",
    "        \n",
    "        df = pd.DataFrame(list(metrics), index=['Precision', 'Recall', 'Fscore', 'support'],\n",
    "                                   columns=ats.text_label)\n",
    "        df = df.drop(['support'], axis=0)\n",
    "        #display(df.T)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNClassifier(\n",
      "  (embedding): Embedding(1502, 50, padding_idx=0)\n",
      "  (rnn): GRU(50, 20, batch_first=True, bidirectional=True)\n",
      "  (fc1): Linear(in_features=40, out_features=5, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_voc_size, embedding_size, hidden_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        self.input_voc_size = input_voc_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_out_size = hidden_size * 2\n",
    "\n",
    "        \n",
    "        self.num_classes = 5\n",
    "        \n",
    "        # Add the padding token (0) (+1 to voc_size)\n",
    "        # Pads the output with the embedding vector at padding_idx whenever it encounters the index..\n",
    "        self.embedding = nn.Embedding(input_voc_size+1, embedding_size, padding_idx=0)\n",
    "        # Load the pretrained embeddings\n",
    "        # self.embedding.weight = nn.Parameter(pretrained_emb_vec) \n",
    "        # embeddings fine-tuning\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "              input_size=embedding_size,\n",
    "              hidden_size=hidden_size,\n",
    "              batch_first=True,\n",
    "              bidirectional=True,\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.rnn_out_size, self.num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    # input shape: B x S (input size)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        vprint(\"\\nsize input\", x.size())\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden (num_layers * num_directions, batch_size, hidden_size)\n",
    "        h_0 = torch.zeros(2, batch_size, self.hidden_size)\n",
    "        vprint(\"size hidden init\", h_0.size())\n",
    "        \n",
    "        # When creating new variables inside a model (like the hidden state in an RNN/GRU/LSTM),\n",
    "        # make sure to also move them to the device (GPU or CPU).\n",
    "        h_0 = h_0.to(device)\n",
    "\n",
    "        # Embedding B x S -> B x S x I (embedding size)\n",
    "        emb = self.embedding(x)\n",
    "        vprint(\"size Embedding\", emb.size())\n",
    "        \n",
    "        # Propagate embedding through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, hidden = self.rnn(emb, h_0)\n",
    "        \n",
    "        vprint(\"size hidden\", hidden.size())\n",
    "        \n",
    "        rnn_out = torch.cat((hidden[0], hidden[1]), 1)\n",
    "        vprint(\"size rnn out\", rnn_out.size())\n",
    "        \n",
    "        \n",
    "        # Use the last layer output as FC's input\n",
    "        layout_fc1 = self.fc1(rnn_out)\n",
    "        vprint(\"size layout fc1\", layout_fc1.size())\n",
    "        \n",
    "        fc_output = self.softmax(layout_fc1)\n",
    "        \n",
    "        return fc_output    \n",
    "    \n",
    "# Add the unknown token (+1 to voc_size)\n",
    "rnn = RNNClassifier(vocabulary_size+1, EMBEDDINGS_SIZE, 20)\n",
    "rnn.to(device)\n",
    "print(rnn)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "weights = [1-((dataset_train.df['score'] == i).sum() / len(dataset_train)) for i in range(5)]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "def force_symlink(file1, file2):\n",
    "    if os.path.exists(file2):\n",
    "        os.remove(file2)\n",
    "    os.symlink(file1, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  1/20] | Step [  852/1142 ( 99%)] | Loss 1.705 | Accuracy 30.21% @ Loss_dev 1.478 | Accuracy_dev 52.82%\n",
      "Epoch [  2/20] | Step [  852/1142 ( 99%)] | Loss 1.368 | Accuracy 32.84% @ Loss_dev 1.574 | Accuracy_dev 15.49%\n",
      "Epoch [  3/20] | Step [  852/1142 ( 99%)] | Loss 1.481 | Accuracy 32.40% @ Loss_dev 1.533 | Accuracy_dev 27.46%\n",
      "Epoch [  4/20] | Step [  852/1142 ( 99%)] | Loss 1.525 | Accuracy 35.29% @ Loss_dev 1.503 | Accuracy_dev 33.80%\n",
      "Epoch [  5/20] | Step [  852/1142 ( 99%)] | Loss 1.624 | Accuracy 39.75% @ Loss_dev 1.423 | Accuracy_dev 49.30%\n",
      "Epoch [  6/20] | Step [  852/1142 ( 99%)] | Loss 1.442 | Accuracy 42.29% @ Loss_dev 1.393 | Accuracy_dev 57.04%\n",
      "Epoch [  7/20] | Step [  852/1142 ( 99%)] | Loss 1.445 | Accuracy 44.40% @ Loss_dev 1.372 | Accuracy_dev 57.75%\n",
      "Epoch [  8/20] | Step [  852/1142 ( 99%)] | Loss 1.478 | Accuracy 46.76% @ Loss_dev 1.328 | Accuracy_dev 62.68%\n",
      "Epoch [  9/20] | Step [  852/1142 ( 99%)] | Loss 1.571 | Accuracy 49.91% @ Loss_dev 1.266 | Accuracy_dev 69.01%\n",
      "Epoch [ 10/20] | Step [  852/1142 ( 99%)] | Loss 1.410 | Accuracy 51.66% @ Loss_dev 1.224 | Accuracy_dev 72.54%\n",
      "Epoch [ 11/20] | Step [  852/1142 ( 99%)] | Loss 1.391 | Accuracy 52.63% @ Loss_dev 1.267 | Accuracy_dev 66.90%\n",
      "Epoch [ 12/20] | Step [  852/1142 ( 99%)] | Loss 1.399 | Accuracy 53.06% @ Loss_dev 1.211 | Accuracy_dev 71.83%\n",
      "Epoch [ 13/20] | Step [  852/1142 ( 99%)] | Loss 1.220 | Accuracy 54.99% @ Loss_dev 1.209 | Accuracy_dev 71.13%\n",
      "Epoch [ 14/20] | Step [  852/1142 ( 99%)] | Loss 1.580 | Accuracy 57.18% @ Loss_dev 1.178 | Accuracy_dev 73.94%\n",
      "Epoch [ 15/20] | Step [  852/1142 ( 99%)] | Loss 1.372 | Accuracy 57.62% @ Loss_dev 1.163 | Accuracy_dev 77.46%\n",
      "Epoch [ 16/20] | Step [  852/1142 ( 99%)] | Loss 1.231 | Accuracy 57.88% @ Loss_dev 1.183 | Accuracy_dev 77.46%\n",
      "Epoch [ 17/20] | Step [  852/1142 ( 99%)] | Loss 1.529 | Accuracy 59.37% @ Loss_dev 1.141 | Accuracy_dev 78.87%\n",
      "Epoch [ 18/20] | Step [  852/1142 ( 99%)] | Loss 1.447 | Accuracy 59.72% @ Loss_dev 1.144 | Accuracy_dev 78.87%\n",
      "Epoch [ 19/20] | Step [  852/1142 ( 99%)] | Loss 1.277 | Accuracy 61.12% @ Loss_dev 1.132 | Accuracy_dev 79.58%\n",
      "Epoch [ 20/20] | Step [  852/1142 ( 99%)] | Loss 1.398 | Accuracy 63.31% @ Loss_dev 1.136 | Accuracy_dev 80.28%\n",
      "Learning finished!\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "\n",
    "num_epochs=20\n",
    "\n",
    "iter = 0\n",
    "iter_batch = 0\n",
    "best_accuracy_dev = 0.0\n",
    "\n",
    "rnn.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_target = 0\n",
    "    train_loss_batches = 0\n",
    "    train_loss_batches_count = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = rnn(data)\n",
    "        \n",
    "        vprint(output)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        \n",
    "        train_loss_batches += loss.cpu().detach().numpy()\n",
    "        train_loss_batches_count += 1\n",
    "        \n",
    "        rnn.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get the Accuracy\n",
    "        _, predicted = torch.max(output.data, dim=1)\n",
    "        correct = (predicted == target).sum().item()\n",
    "        \n",
    "        total_correct += correct\n",
    "        total_target += target.size(0)\n",
    "        \n",
    "        if batch_idx % 200 == 0 or batch_idx % 200 == 1 or batch_idx == len(train_loader)-1:\n",
    "            print('\\rEpoch [{:3}/{}] | Step [{:5}/{} ({:3.0f}%)] | Loss {:.3f} | Accuracy {:.2f}%'.format(\n",
    "                    epoch+1, num_epochs,\n",
    "                    batch_idx * len(data), \n",
    "                    len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), \n",
    "                    loss.item(), \n",
    "                    (total_correct / total_target) * 100), end=' ')\n",
    "            \n",
    "            writer.add_scalar('data/loss/_train_only', train_loss_batches / train_loss_batches_count, iter_batch)\n",
    "            iter_batch += 1\n",
    "            \n",
    "        if Verbose:\n",
    "            break\n",
    "            \n",
    "    accuracy_dev, loss_dev = evaluate(rnn, dev_loader, criterion=criterion, whileTraining=True)\n",
    "    print(\"@ Loss_dev {:.3f} | Accuracy_dev {:.2f}%\".format(loss_dev, accuracy_dev))\n",
    "    \n",
    "    if best_accuracy_dev < accuracy_dev:\n",
    "        best_accuracy_dev = accuracy_dev\n",
    "        file = 'checkpoint.pth.' + str(epoch) + '.acc.' + str(round(accuracy_dev,2)) + '.tar'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': rnn.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, file)\n",
    "        force_symlink(file, 'checkpoint.pth.best.tar')\n",
    "\n",
    "    writer.add_scalars('data/loss/evol', {'train': train_loss_batches / train_loss_batches_count,\n",
    "                                         'dev': loss_dev}, iter)\n",
    "    iter += 1\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint epoch 18\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint.pth.best.tar')\n",
    "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"=> loaded checkpoint epoch {}\"\n",
    "      .format(checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume 1  got a score of Score of: 3\n",
      "Resume 2  got a score of Score of: 3\n",
      "Resume 3  got a score of Score of: 5\n",
      "Wall time: 517 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_dev = pd.read_csv(\"C:/Users/Rajath/Downloads/sentence-entailment-master/sentence-entailment-master/test4.csv\",sep ='$', encoding = \"ISO-8859-1\")\n",
    "dataset_dev = ats(df_dev, vocabulary_size, dictionary_train)\n",
    "dev_loader = DataLoader(dataset=dataset_dev,\n",
    "                         batch_size=1, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "evaluate(rnn, dev_loader, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
